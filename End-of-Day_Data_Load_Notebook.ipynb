{"nbformat_minor": 2, "cells": [{"source": "### Reading Trade partition dataset from its temporary location", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 1, "cell_type": "code", "source": "trade_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=T/*.parquet\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1613489928550_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-guided.21k4sb3f4jfuhddfwtlmb5hkea.bx.internal.cloudapp.net:8088/proxy/application_1613489928550_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn5-guided.21k4sb3f4jfuhddfwtlmb5hkea.bx.internal.cloudapp.net:30060/node/containerlogs/container_1613489928550_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 5702.548095703125, "end_time": 1613494646808.559}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Selecting the Necessary columns for Trade Records", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 40.43701171875, "end_time": 1613495343645.237}}, "editable": true, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "trade = trade_common.select(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\",\"event_seq_nb\", \"arrival_tm\", \"trade_pr\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 246.621826171875, "end_time": 1613494656439.026}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "### Applying Data Correction", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 40.390869140625, "end_time": 1613495753137.653}}, "editable": true, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "from pyspark.sql.window import Window\nfrom datetime import date\nimport pyspark.sql.functions as F \n\n\n# In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n# symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n# any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n\n# Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n\ntrade_corrected=trade.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(trade.trade_dt,\\\n                   trade.symbol,trade.exchange,trade.event_tm,trade.event_seq_nb) \\\n                   .orderBy(trade.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 776.636962890625, "end_time": 1613494658939.564}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "### Writing the Trade Dataset back to Azure Storage", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "trade_date = \"2020-08-05\"\ntrade_corrected.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-02-16t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/trade/trade_dt={}\".format(trade_date))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 13356.571044921875, "end_time": 1613494673095.803}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Reading the Quote partition dataset from its temporary location", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "quote_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=Q/*.parquet\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 751.6689453125, "end_time": 1613494879270.603}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "### Selecting the Necessary columns for Quote Records", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "quote=quote_common.select(\"trade_dt\",\"symbol\",\"exchange\",\"event_tm\",\"event_seq_nb\",\"arrival_tm\",\"bid_pr\",\"ask_pr\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 238.382080078125, "end_time": 1613494942934.174}}, "collapsed": true}}, {"execution_count": 9, "cell_type": "code", "source": "from pyspark.sql.window import Window\nfrom datetime import date\nimport pyspark.sql.functions as F \n\n# In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n# symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n# any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n\n# Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n\nquote_corrected=quote.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(quote.trade_dt,quote.symbol,\\\n                                            quote.exchange,quote.event_tm,quote.event_seq_nb).\\\n                                            orderBy(quote.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 246.822998046875, "end_time": 1613495115098.263}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Writing back the Quote Dataset back to Azure Storage", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "trade_date = \"2020-08-05\"\nquote.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-02-16t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/quote/trade_dt={}\".format(trade_date))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2260.718017578125, "end_time": 1613495123897.714}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}